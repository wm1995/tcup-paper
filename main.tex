% main.tex
%
% LaTeX template for creating an MNRAS paper
%
% v3.0 released 14 May 2015
% (version numbers match those of mnras.cls)
%
% Copyright (C) Royal Astronomical Society 2015
% Authors:
% Keith T. Smith (Royal Astronomical Society)

% Change log
%
% v3.0 May 2015
%    Renamed to match the new package name
%    Version number matches mnras.cls
%    A few minor tweaks to wording
% v1.0 September 2013
%    Beta testing only - never publicly released
%    First version: a simple (ish) template for creating an MNRAS paper

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Basic setup. Most papers should leave these options alone.
\documentclass[fleqn,usenatbib]{mnras}

% MNRAS is set in Times font. If you don't have this installed (most LaTeX
% installations will be fine) or prefer the old Computer Modern fonts, comment
% out the following line
\usepackage{newtxtext,newtxmath}
% Depending on your LaTeX fonts installation, you might get better results with one of these:
%\usepackage{mathptmx}
%\usepackage{txfonts}

% Use vector fonts, so it zooms properly in on-screen viewing software
% Don't change these lines unless you know what you are doing
\usepackage[T1]{fontenc}

% Allow "Thomas van Noord" and "Simon de Laguarde" and alike to be sorted by "N" and "L" etc. in the bibliography.
% Write the name in the bibliography as "\VAN{Noord}{Van}{van} Noord, Thomas"
\DeclareRobustCommand{\VAN}[3]{#2}
\let\VANthebibliography\thebibliography
\def\thebibliography{\DeclareRobustCommand{\VAN}[3]{##3}\VANthebibliography}


%%%%% AUTHORS - PLACE YOUR OWN PACKAGES HERE %%%%%

% Only include extra packages if you really need them. Common packages are:
\usepackage{graphicx}	% Including figure files
\usepackage{amsmath}	% Advanced maths commands
% \usepackage{amssymb}	% Extra maths symbols

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%% AUTHORS - PLACE YOUR OWN COMMANDS HERE %%%%%

% Please keep new commands to a minimum, and use \newcommand not \def to avoid
% overwriting existing commands. Example:
%\newcommand{\pcm}{\,cm$^{-2}$}	% per cm-squared

\newcommand{\studentt}[2]{t_\nu \left( #1, #2 \right)}
\newcommand{\depvar}{y_i}
\newcommand{\indepvars}{\boldsymbol{x}_i}
\newcommand{\obsdep}{\hat{y}_i}
\newcommand{\obsindep}{\hat{\boldsymbol{x}}_i}
\newcommand{\obserr}{\Sigma_i}
\newcommand{\intscttr}{\sigma_{\text{int}}}
\newcommand{\intercept}{\alpha}
\newcommand{\covariate}{\beta}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%% TITLE PAGE %%%%%%%%%%%%%%%%%%%

% Title of the paper, and the short title which is used in the headers.
% Keep the title short and informative.
\title[Robust regression for astronomy]{Robust regression with measurement errors in an astronomical context}

% The list of authors, and the short list which is used in the headers.
% If you need two or more lines of authors, add an extra line using \newauthor
\author[W. Martin et al.]{
William Martin$^{1}$\thanks{E-mail: w.martin19@imperial.ac.uk}
and Daniel Mortlock$^{1,2,3}$
\\
% List of institutions
$^{1}$Department of Physics, Imperial College London, Blackett Laboratory, Prince Consort Road, London SW7 2AZ, UK\\
$^{2}$Department of Mathematics, Imperial College London, London, SW7 2AZ, UK\\
$^{3}$Department of Astronomy, The Oskar Klein Centre, Stockholm University, Albanova, SE-10691 Stockholm, Sweden
}

% These dates will be filled out by the publisher
\date{Accepted XXX. Received YYY; in original form ZZZ}

% Enter the current year, for the copyright statements etc.
\pubyear{2023}

% Don't change these lines
\begin{document}
\label{firstpage}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\maketitle

% Abstract of the paper
\begin{abstract}
A Bayesian hierarchical model for robust linear regression in the presence of measurement error is described.
The performance of the model is assessed on both simulated and real datasets.
The code is made available for others to use.
\end{abstract}

% Select between one and six entries from the list of approved keywords.
% Don't make up new ones.
\begin{keywords}
methods: statistical -- methods: data analysis -- software: data analysis
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% BODY OF PAPER %%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:intro}

{\color{red}

(Each point is ~1 paragraph.)

Regression generally important in astronomy (e.g., M-$\sigma$)

Astronomy data well characterised errors, but with some outliers

Previously sigma-clipping or outlier removal; better to model the full measurement process

Review of previous methods (e.g., Kelly)

Characterise as an example of robust inference (i.e., don't need to know the actual generating distribution); mention some existing uses of this (e.g., Feeney, Park)

Then say presenting practical methodology and implementation here.
}

There are multiple approaches to the problem of linear regression in astronomy
where all variables are measured with error.  \citet{Kelly:2007} illustrates
that ad-hoc estimators such as \textsc{fitexy} \citep{Press:1992, Tremaine:2002}
and \textsc{bces} \citep{Akritas:1996} suffer from biases and can underestimate
intrinsic scatter. As an alternative, Kelly presents a Bayesian hierarchical
model for linear regression with measurement errors as an alternative. However,
the model is designed to be sampled using Gibbs sampling, which limits the
choice of distributions for variables within the model and the number of
parameters that can be fit simultaneously. As a result of the former, the model
assumes random variables are normally distributed throughout; however, inference
that relies on normal distributions can be unduly affected by outliers (see
\citet{Berger:1994} for a review of robust Bayesian inference).  One possible
alternative is to use Student's $t$-distributions, which have heavier tails and
are, therefore, less affected by the presence of outliers \citep{Berger:1994};
Student's $t$-distributions have seen use in astronomical
\citep[][e.g.]{Park:2017} and cosmological \citep[][e.g.]{Feeney:2018}
inference.

Bayesian hierarchical models (BHMs) are a natural way to deal with datasets with
measurement errors, selection effects, interlinked parameters, censored data,
and many other effects common to astronomical problems. BHMs have seen
increasing use in astrophysics over the past 30 years, from the
distance-redshift relation in cosmology \citep[e.g.][]{Feeney:2018,
Avelino:2019} and photometric redshift estimation \citep[e.g.][]{Leistedt:2016}
to exoplanet characterisation \citep[e.g.][]{Sestovic:2018} and population-level
inference \citep[e.g.][]{Kelly:2009} --- see \citet{Feigelson:2021} for a recent
review of statistical methods in astrophysics, including further examples of
BHMs.

\citet{Kelly:2007} formulated a Bayesian linear regression model under the
assumption of Gaussianity and compared it with other common regression methods
in astronomy, including ordinary least squares, \textsc{bces}
\citep{Akritas:1996}, and \textsc{fitexy} \citep{Press:1992, Tremaine:2002}.
\citeauthor{Kelly:2007} found that the Bayesian approach had several advantages
over the other methods considered: no bootstrapping was required to obtain
uncertainties on parameters; the Bayesian approach was easily extensible to
truncated or censored data; and other methods would sometimes severely
underestimate intrinsic scatter in the data. This formulation of Bayesian
regression, sometimes known as \textsc{linmix\_err}, is now commonly used in
astronomy \citep[e.g.][]{McConnell:2013, Bentz:2013, Andrews:2013}.

When we have a dataset that has a large number of outliers, our inference of
certain parameters can be drawn off by the presence of those outliers. Robust
inference refers to a body of techniques to fit in an outlier-resistant way so
that we can recover the parameters of interest while not being unduly affected
by the outliers.  For example, ordinary least squares regression is based on the
assumption that datapoints are normally distributed about a true regression
line; however, normal distributions have exponential tails, which means that
this approach is often heavily affected by the presence of outliers (see Section
\ref{sec:results} for an exploration of this effect). An alternative approach is
to use heavier-tailed distributions.  Examples of this include least absolute
deviation regression, where the data is assumed to be Laplace-distributed about
the regression line.  Another distribution that can be used is Student's
$t$-distribution \citep[e.g.][]{Berger:1994, Gelman:2013}, which also has
heavier tails than a normal distribution --- see Section \ref{sec:results} for a
comparison of regression using normal distributions and $t$-distributions.

There are several approaches to robust inference in a Bayesian context
--- see \citet{Berger:1994} for a review. Examples of applications of robust
inference in astronomy include \citet{Park:2017, Feeney:2018}.

From the review of previous methods, we can identify properties that we would
like to see in our regression model:

\begin{itemize}
	\item A Bayesian hierarchical model -- we favour a hierarchical approach
	because it naturally encodes the hierarchical structure of astronomical
	regression problems (i.e.\ objects are drawn from a high-level population;
	the objects intrinsically obey some relationship; the objects are measured
	with error and only the measured values are known). We favour a Bayesian
	approach because it gives posterior distributions encoding our degree of
	belief of the values of parameters, quantifying our uncertainty in their
	values.

	\item A robust model -- we desire a model that is robust to both outliers
	(i.e.\ data points that, whether intrinsically or by virtue of measurement
	errors, do not lie on our regression relation) and model misspecification
	(i.e.\ where the underlying distribution of data does not match up with the
	distribution assumed for modelling).

	\item A general method -- we seek a model that does not require case-by-case
	optimisation for application to different regression problems (e.g.\ no
	manual outlier identification and removal, no need to rescale prior
	distributions for different problems, etc.)
\end{itemize}

\section{Formalism}
\label{sec:formalism}

Let us observe $N$ objects, each with $\{\hat{x}_i, \hat{y}_i,
\boldsymbol{\Sigma}_i\}$, representing the independent variable, the dependent
variable, and the covariance matrix of the observed quantities.  We can further
assume that each observation represents a true-but-unobserved latent object
$\{x_i, y_i\}$, with these quantities related by
\begin{eqnarray}
    y_i &=& \alpha x_i + \beta + \epsilon_i \\
    \epsilon_i &\sim& \mathcal{P} \left( \sigma_{\text{int}} \right)
\end{eqnarray}
for regression parameters $\boldsymbol{\theta} = \left\{\alpha, \beta,
\sigma_{\mathrm{int}}\right\}$ and some probability distribution $P\left(
\hat{\mathbf{x}}, \hat{\mathbf{y}} \mid \mathbf{x}, \mathbf{y}, \Sigma \right)$
linking the latent values with the observed values.

\subsection{Regression model}
\label{sec:formalism.model}

Similarly to \citet{Kelly:2007}, we define a Bayesian hierarchical model to
reflect the nature of the regression problem. Firstly, we assume that the
dependent variable $\{\depvar\}$ and the independent variables $\{\indepvars\}$
have a linear relationship with some underlying scatter $\intscttr$ -- i.e.

\begin{equation}
\depvar \sim \studentt{\covariate \indepvars + \intercept}{\intscttr}.
\end{equation}

We use a Student's $t$-distribution here not because we believe that the data
intrinsically follows this distribution, but because the resulting model is more
robust to outliers at the object level -- such as if a galaxy were to be an
outlier from a particular relation as a result of a recent merger.

The quantities $\{\depvar\}$ and $\{\indepvars\}$ are, however, rarely known
exactly; instead, we measure the observables $\{\obsdep\}$ and $\{\obsindep\}$
with covariance $\{\obserr\}$ in the most general case. This measurement is
modelled as

\begin{equation}
    \begin{pmatrix}
        \obsdep \\ \obsindep
    \end{pmatrix}
    \sim
    \studentt{
        \begin{pmatrix}
            \depvar \\ \indepvars
        \end{pmatrix}
    }{
        \obserr
    }.
\end{equation}

Once again, we use a Student's $t$-distribution because the resulting model is
more robust to outliers -- in this case, the object itself is not an outlier
from a relation, but the observed quantities are outliers as a result of their
measurement.

\begin{figure}
	\includegraphics[width=\columnwidth]{graphics/dag.pdf}
    \caption{A directed acyclic graph representing the $t$-cup model.}
    \label{fig:formalism.dag}
\end{figure}

\subsection{Sampling distribution}
\label{sec:formalism.sampling}

Student's $t$-distributions are encountered when estimating the mean of a normal
distribution with unknown variance from a limited number of samples. The number
of samples is a parameter of the distribution: for $n$ samples, the
corresponding Student's $t$-distribution will have $\nu \equiv n - 1$
``degrees-of-freedom''. This value of $\nu$ parameterises how heavy-tailed the
distribution is. While the interpretation of $\nu$ as ``degrees-of-freedom''
only makes sense for $\nu \in \mathbb Z^+$, the distribution is normalisable for
any positive, real $\nu$; for this reason, we shall refer to $\nu$ as the shape
parameter. Constructing a statistical model that treats this shape parameter as
a free parameter to be fitted allows the model to adapt to outliers in the data,
reverting to the result of a model that uses normal distributions in the absence
of outliers \citep{Feeney:2018}. In this context, $\nu$ is considered a
nuisance parameter: its exact value is unimportant, and its inclusion in the
model is solely to ensure that inference is robust to outliers.

The Student's $t$-distribution has probability density function
\begin{equation}
    \studentt{\mu}{\sigma}
        =
    \frac{1}{\sqrt{\pi \nu} \sigma}
    \frac{
        \Gamma \left(\frac{\nu + 1}2\right)
    }{
        \Gamma \left(\frac{\nu}2\right)
    }
    \left(
        1 + \frac{1}{\nu} \frac{\left(x - \mu\right)^2}{\sigma^2}
    \right)^{
        -\frac{\nu + 1}{2}
    }.
\end{equation}
Plots of a range of $\nu$ are shown in Figure \ref{fig:model.t}; note that $\nu
= 1$ gives a Cauchy distribution, and $\nu \rightarrow \infty$ tends to a normal
distribution. The distribution has mean
\begin{equation}
    \mathbb{E}(x)
        =
    \begin{cases}
        \mu & \nu > 1, \\
        \textrm{undefined} & \textrm{otherwise}
    \end{cases}
\end{equation}
and variance
\begin{equation}
    \mathrm{Var}(x)
        =
    \begin{cases}
        \frac{\nu}{\nu - 2} \sigma^2 & \phantom{2 \leq} \nu > 2, \\
        \infty & 2 \leq \nu > 1, \\
        \textrm{undefined} & \textrm{otherwise.}
    \end{cases}
\end{equation}

\subsubsection{Priors on $t$-distribution parameters}
\label{sec:formalism.sampling.prior}

It can be informative to reparameterise the shape parameter $\nu$ in a few
different ways. Following \citet{Feeney:2018}, we can define the peak height
ratio $t$ as
\begin{eqnarray}
    t(\nu) &=& \frac{
        \studentt{x = \mu; \mu}{\sigma}
    }{
        \mathcal N \left(x = \mu; \mu, \sigma \right)
    } \\
    &=& \sqrt{\frac{2}{\nu}} \;
    \frac{
        \Gamma\left(\frac{\nu + 1}{2}\right)
    }{
        \Gamma\left(\frac{\nu}{2}\right)
    }.
\end{eqnarray}
As can be seen in Figure \ref{fig:model.peak_height}, the behaviour of $t(\nu)$
is highly non-linear, reflecting that, for $\nu \gtrsim 10$, the Student's
$t$-distribution rapidly approaches a normal distribution.

% \begin{figure}
%     \centering
%     \includegraphics{scripts/peak-height.pdf}
%     \caption{A plot of peak height ratio $t$ with varying shape parameter $\nu$}
%     \label{fig:model.peak_height}
% \end{figure}

As we are using the flexibility of Student's $t$-distributions to adapt to
datasets with outliers, it can also be useful to consider the outlier fraction
$\omega$. If we define outliers as any points lying more than $3\sigma$ from the
mean $\mu$, the outlier fraction is
\begin{eqnarray}
    \omega(\nu)
    &=& P\left(
        \left|x - \mu \right| > 3 \sigma \;
        \middle| \;
        x \sim \studentt{\mu}{\sigma}
    \right) \\
    &=& 2 F_\nu \left(\mu - 3 \sigma \right),
\end{eqnarray}
where $F_\nu(x)$ is the cumulative distribution function for Student's
$t$-distribution with shape parameter $\nu$. Figure \ref{fig:model.outlier_frac}
shows the relationship between outlier fraction $\omega$ and shape parameter
$\nu$, with $\omega \rightarrow 2.70 \times 10^{-3} $ in the limit of a normal
distribution --- i.e., as $\nu \rightarrow \infty$. We would, therefore, expect
that approximately 1 in 370 datapoints would be outliers for data that follows a
normal distribution\footnotemark; for Cauchy-distributed data (i.e. $\nu = 1$),
we would expect every fifth datapoint to be an outlier.

\footnotetext{As of the time of writing, we know of $\sim$300 $z > 6$ quasars
\citep{Bosman:2020}; under the assumption of normality, we would expect no
outliers in this dataset.}

% \begin{figure}
%     \centering
%     \includegraphics{scripts/outlier-frac.pdf}
%     \caption{Outlier fraction $\omega$ as a function of $\nu$ - for large $\nu$,
%     $\omega$ tends to the Gaussian limit value of $\omega \rightarrow 2.70
%     \times 10^{-3}$}
%     \label{fig:model.outlier_frac}
% \end{figure}

Using Student's $t$-distributions for robust Bayesian inference means that we
have to specify a prior on the shape parameter $\nu$. One approach is to adopt a
fixed value of $\nu$, which is equivalent to setting a Dirac delta function
prior: a common choice is $\nu = 4$ \citep[e.g.][]{Berger:1994, Gelman:2013}.
Another approach is to adopt a more flexible approach by allowing $\nu$ to vary
\citep[e.g.][]{Juarez:2010, Park:2017, Feeney:2018}.

In \citet{Feeney:2018}, it is argued that, for common choices of prior, too
much prior density is placed on large values of $\nu$; to rectify this, they
propose a uniform prior on peak height $t \sim U\left(0, 1\right)$. As this
prior has no closed form derivative (a requirement for Hamiltonian Monte Carlo),
they approximate this with the prior
\begin{equation}
    \mathcal P\left(\nu\right) \propto
    \frac{
        \Theta(\nu)
    }{
        \left(
            \left(\frac{\nu}{\nu_0}\right)^{1 / (2 a)}
            + \left(\frac{\nu}{\nu_0}\right)^{2 / a}
        \right)^a
    }
\end{equation}
where $\Theta(\nu)$ is the Heaviside step function, and $\nu_0$ and $a$ are
constants with values $\sim 0.55$ and $\sim 1.2$ respectively.  One feature of
this prior is that it has significant density for $0 < \nu < 1$; in extreme
cases, this can correspond to outliers that are more than 100 orders of
magnitude larger than $\sigma$. In theory, these unphysical regions of parameter
space ought to be excluded during the process of inference. However, the use of
Hamiltonian Monte Carlo (HMC) in such cases can lead to divergences in the
sampling process or inefficient sampling as the sampler struggles with regions
of high curvature --- see Section \ref{sec:model.model.reparam} for greater
detail.

Another possible prior is on outlier fraction $\omega$. It could be argued that
the term outlier loses its meaning when the majority of a dataset is composed of
so-called ``outliers''; therefore, a natural choice of prior might be a uniform
distribution ranging from the normally-distributed outlier fraction of
$\sim$0.00270 to this ``maximum'' outlier fraction of 0.5, corresponding to $\nu
\sim 0.302$. This still leads to large outliers (some of 8 orders of magnitude
for 100 draws from the distribution), which are unphysical and continue to
present difficulties when sampling with HMC.

To limit the number of unphysical outliers, we can instead limit the prior to
consider only distributions that are less heavy-tailed than the Cauchy
distribution --- i.e. all those with $\nu > 1$. This prior can be set on either
peak height $t$ or outlier fraction $\omega$.

% \begin{figure}
%     \centering
%     \includegraphics{scripts/scaled-peak-height.pdf}
%     \caption{Rescaled peak height $\tilde{t}$ as a function of $\nu$; also
%     illustrated is the approximation given in Equation \ref{eqn:model.t_approx}.
%     }
%     \label{fig:model.t_approx}
% \end{figure}

After considering a range of priors, we have adopted a prior which is
approximately uniform on peak height between Cauchy and normal distributions.
We achieve this by rescaling peak height $t$ to $\tilde{t}$, such that
$\tilde{t} = 0$ corresponds to a Cauchy distribution, and $\tilde{t} = 1$
corresponds to a normal distribution. We adopt a uniform prior $\tilde{t} \sim
U\left(0, 1\right)$, and then approximate the function
\begin{equation}
    \nu\left(\tilde{t}\right)
        \approx
    \sec \left(\frac{\pi}{2} \sqrt{\tilde{t}} \right);
    \label{eqn:model.t_approx}
\end{equation}
this approximation (illustrated in Figure \ref{fig:model.t_approx}) is accurate
to within 3\% and provides an analytic derivative for HMC.

\subsection{Asymptotic normality}
\label{sec:formalism.asymptotic}

{\color{green} For Daniel}

\subsection{Implementation}
\label{sec:formalism.implementation}

The chosen statistical model is implemented in Stan \citep{Stan}, which is used
to draw samples via a HMC No U-Turn Sampler. The implementation is packaged as
$t$-cup, available as a Python package\footnotemark.

For the purposes of comparison, we have also implemented a model that mirrors
the above structure, but uses normal distributions at each stage --- this model
is referred to as $n$-cup.

\footnotetext{\url{https://github.com/wm1995/tcup}}

% Normally the next section describes the techniques the authors used.
% It is frequently split into subsections, such as Section~\ref{sec:maths} below.

% \subsection{Maths}
% \label{sec:maths} % used for referring to this section from elsewhere

% Simple mathematics can be inserted into the flow of the text e.g. $2\times3=6$
% or $v=220$\,km\,s$^{-1}$, but more complicated expressions should be entered
% as a numbered equation:

% \begin{equation}
%     x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}.
% 	\label{eq:quadratic}
% \end{equation}

% Refer back to them as e.g. equation~(\ref{eq:quadratic}).

% \subsection{Figures and tables}

% Figures and tables should be placed at logical positions in the text. Don't
% worry about the exact layout, which will be handled by the publishers.

% Figures are referred to as e.g. Fig.~\ref{fig:example_figure}, and tables as
% e.g. Table~\ref{tab:example_table}.

\section{Results on simulated datasets}
\label{sec:results}

In the previous section, we proposed a general-purpose, robust statistical model
for linear regression; in this section, we investigate the performance of the
model on a series of simulated datasets with known parameters. Code that
reproduces the datasets in this section is available online\footnotemark.

\footnotetext{\url{https://github.com/wm1995/tcup-paper}}

\subsection{\textit{t}-distributed data}
\label{sec:results.t}

Our first test illustrates a dataset that matches our model perfectly, with $N =
20$ datapoints drawn from the following distribution:
\begin{eqnarray}
    x_i &\sim& \mathcal N (\mu = 2, \sigma = 2) \\
    y_i &\sim& t_{3} (\mu = 3 + 2 x_i, \sigma = 0.1) \\
    \log_{10} \sigma_{x, i} &\sim& \mathcal N (\mu = -1, \sigma = 0.1) \\
    \log_{10} \sigma_{y, i} &\sim& \mathcal N (\mu = -0.7, \sigma = 0.1) \\
    \hat{x}_i &\sim& t_{3} (\mu = x_i, \sigma = \sigma_{x, i}) \\
    \hat{y}_i &\sim& t_{3} (\mu = y_i, \sigma = \sigma_{y, i}).
\end{eqnarray}

Note that $\nu = 3$ corresponds to an outlier fraction of $\omega(\nu = 3)
\approx 5.8 \%$.

\begin{figure}
    \includegraphics[width=\columnwidth]{example-image-a}
    \caption{{\color{red} 100} draws from the posterior of regression lines
    (red) for the \textit{t}-distributed dataset (black points). The ground
    truth regression line is illustrated by the black dashed line.}
    \label{fig:results.t.regression}
\end{figure}

\begin{figure}
    \includegraphics[width=\columnwidth]{example-image-a}
    \caption{The posterior for each of the regression coefficients and the shape
    parameter $\nu$ for the \textit{t}-distributed dataset, with ground-truth
    values indicated by the black dashed lines.}
    \label{fig:results.t.corner}
\end{figure}

As we can see in Figures \ref{fig:results.t.regression} and
\ref{fig:results.t.corner}, we recover the values of the parameters that were
used to generate the dataset.

We then generated 1000 datasets from the same ground-truth parameters, and
calculated the maximum a posteriori (MAP) values of the regression coefficients
and shape parameter $\nu$. The results (illustrated in Figure
\ref{fig:results.t.map}) indicate that the estimate of true parameter values is
unbiased.{
\color{red} We also calculated the 95\% credible intervals for each parameter,
finding that the true parameter value was contained within the interval for
$x$\% of parameters and runs.
}

\begin{figure}
    \includegraphics[width=\columnwidth]{example-image-a}
    \caption{The distribution of maximum a posteriori (MAP) estimates for each
    of the regression parameters, as well as the shape parameter $\nu$.}
    \label{fig:results.t.map}
\end{figure}

\subsection{Normally-distributed data}
\label{sec:results.outlier}

In this test, we compare $t$-cup with an equivalent model that employs normal
distributions to illustrate that:
\begin{enumerate}
    \item $t$-cup is able to reduce to a normal model in the absence of outliers
    \item $t$-cup is able to give less biased results when an extreme outlier is
          introduced.
\end{enumerate}

We generated a dataset of $N = 12$ points using the model:
\begin{alignat}{1}
    x_i& \sim \mathcal N (\mu = 5, \sigma = 3) \\
    y_i& \sim
    \begin{cases}
        \mathcal N (\mu = 3 + 2 x_i - 10, \sigma = 0.2) &
            \text{for the second-largest $x_i$} \\
        \mathcal N (\mu = 3 + 2 x_i, \sigma = 0.2) &
            \text{otherwise} \\
    \end{cases}\\
    \log_{10} \sigma_{x, i}& \sim \mathcal N (\mu = -0.5, \sigma = 0.1) \\
    \log_{10} \sigma_{y, i}& \sim \mathcal N (\mu = -0.3, \sigma = 0.1) \\
    \hat{x}_i& \sim \mathcal N (\mu = x_i, \sigma = \sigma_{x, i}) \\
    \hat{y}_i& \sim \mathcal N (\mu = y_i, \sigma = \sigma_{y, i}).
\end{alignat}

\begin{figure*}
    \includegraphics[width=\linewidth]{example-image-a}
    \caption{{\color{red} 100} draws from the posterior of regression lines from
    the normal model (left panel, blue) and $t$-cup (right panel, red). The
    dataset is illustrated by the black points, with the ground-truth regression
    line illustrated by the black dashed line.}
    \label{fig:results.outlier.regression}
\end{figure*}

\begin{figure}
    \includegraphics[width=\columnwidth]{example-image-a}
    \caption{The posterior for each of the regression coefficients under the
    normal model with the outlier included (dark blue, solid) and excluded
    (light blue, dashed), and for $t$-cup with the outlier included (dark red,
    solid). Ground-truth values are indicated by the black dashed lines.}
    \label{fig:results.outlier.corner}
\end{figure}

\begin{figure}
    \includegraphics[width=\columnwidth]{example-image-a}
    \caption{The posterior for each of the regression coefficients and the shape
    parameter $\nu$ under the $t$-cup model with the outlier included (dark red,
    solid) and excluded (light red, dashed).}
    \label{fig:results.outlier.tcup}
\end{figure}

Figure \ref{fig:results.outlier.regression} illustrates how the estimates for
the true parameter models are biased in the normal model, but less affected in
the $t$-cup model.

In Figure \ref{fig:results.outlier.corner}, we compare the constraints on
parameters derived under the normal model (including and excluding the outlier
from the dataset), and the $t$-cup model (including the outlier only). The
constraints from the $t$-cup model including the outlier are consistent with
those derived under the normal model when the outlier is excluded, obviating the
need to remove the outlier manually. While this outlier is particularly extreme,
this example illustrates the utility of the $t$-cup model in datasets with
outliers.

For completeness, in Figure \ref{fig:results.outlier.tcup} we illustrate that
the $t$-cup model recovers consistent constraints whether the outlier is
included or excluded.

\begin{figure}
    \includegraphics[width=\columnwidth]{example-image-a}
    \caption{The distribution of maximum a posteriori (MAP) estimates for each
    of the regression parameters under the normal model (blue) and $t$-cup model
    (red).}
    \label{fig:results.outlier.map}
\end{figure}

We then generated 1000 datasets using the same procedure, and
calculated the maximum a posteriori (MAP) values of the regression coefficients
for both the normal model and for $t$-cup. The results (illustrated in Figure
\ref{fig:results.outlier.map}) indicate that {\color{red} constraints under
$t$-cup are significantly less biased than those calculated under the normal
model. We also compared the 95\% credible intervals for each parameter,
finding that the true parameter value was contained within the interval for
$x$\% of parameters and runs.
}

\subsection{2D Normal Mixture Model with outlier fraction}
\label{sec:results.gmm}

This test examines how the model performs when there is a significant fraction
of outliers. The intrinsic scatter distribution is a mixture of two normal
distributions with zero mean; 90\% of points are drawn from a core distribution
with standard deviation $\sigma_\textrm{int}$, and 10\% of points are drawn from
an outlier distribution with standard deviation $10 \sigma_\textrm{int}$. We
introduce the random variable $O_i$ to indicate whether the $i$th datapoint is
drawn from the core distribution (in which case, $O_i = 0$) or from the outlier
distribution (for which $O_i = 1$).

We generated a dataset of $N = 200$ points using the model:
\begin{alignat}{1}
    \boldsymbol{x}_i& \sim
    \begin{cases}
        \mathcal N \left(
            \mu = \begin{pmatrix} -3 \\ 2 \end{pmatrix},
            \sigma = \begin{pmatrix} 0.5 & -1 \\ -1 & 4 \end{pmatrix}
        \right) &
            1 \leqslant i \leqslant 140 \\
        \mathcal N \left(
            \mu = \begin{pmatrix} -1 \\ -1 \end{pmatrix},
            \sigma = \begin{pmatrix} 1 & 0.2 \\ 0.2 & 0.8 \end{pmatrix}
        \right) &
            140 < i \leqslant 200 \\
    \end{cases}\\
    O_i& \sim \mathrm{Bernoulli}(0.1) \\
    y_i& \sim
    \begin{cases}
        \mathcal N (\mu = 2 + (3, 1)^T \cdot x_i, \sigma = 0.4) &
            O_i = 0 \\
        \mathcal N (\mu = 2 + (3, 1)^T \cdot x_i, \sigma = 4.0) &
            O_i = 1 \\
    \end{cases}\\
    \Sigma_{x, i}& \sim \mathcal W_2 (\boldsymbol{V} = 0.1 \mathbb{I}, n = 3) \\
    \log_{10} \sigma_{y, i}& \sim \mathcal N (\mu = -1, \sigma = 0.1) \\
    \hat{x}_i& \sim \mathcal N (\mu = \boldsymbol{x}_i, \sigma = \Sigma_{x, i}) \\
    \hat{y}_i& \sim \mathcal N (\mu = y_i, \sigma = \sigma_{y, i}).
\end{alignat}

\begin{figure}
    \includegraphics[width=\columnwidth]{example-image-a}
    \caption{The posterior for each of the regression coefficients and the
    outlier fraction $\omega$ for the normal mixture model dataset, with
    ground-truth values indicated by the black dashed lines. Note that, although
    there is not a one-to-one correspondence between the outlier fraction
    $\omega$ and the $p = 0.1$ outlier fraction in the Bernoulli distribution,
    {\color{red} insert observation here}.}
    \label{fig:results.gmm.corner}
\end{figure}

\subsection{Laplace-distributed data}
\label{sec:results.laplace}

For this dataset, a Laplace distribution was used as the noise distribution, as
it is heavy-tailed but does not fall within the family of $t$ distributions and,
therefore, gives an example of performance under explicit model
misspecification.

We generate $N = 25$ datapoints under the following model:
\begin{eqnarray}
    x_i &\sim& \mathcal U (-5, 5) \\
    y_i &\sim& \mathrm{Laplace} (\mu = -1 + 0.8 x_i, b = 0.2) \\
    \log_{10} \sigma_{x, i} &\sim& \mathcal N (\mu = -1, \sigma = 0.1) \\
    \log_{10} \sigma_{y, i} &\sim& \mathcal N (\mu = -1, \sigma = 0.1) \\
    \hat{x}_i &\sim& \mathcal N (\mu = x_i, \sigma = \sigma_{x, i}) \\
    \hat{y}_i &\sim& \mathcal N (\mu = y_i, \sigma = \sigma_{y, i}).
\end{eqnarray}

\begin{figure}
    \includegraphics[width=\columnwidth]{example-image-a}
    \caption{{\color{red} 100} draws from the posterior of regression lines
    (red) for the Laplace-distributed dataset (black points). The ground
    truth regression line is illustrated by the black dashed line.}
    \label{fig:results.laplace.regression}
\end{figure}

\begin{figure}
    \includegraphics[width=\columnwidth]{example-image-a}
    \caption{The posterior for each of the regression coefficients and the shape
    parameter $\nu$ for the Laplace-distributed dataset, with ground-truth
    values indicated by the black dashed lines.}
    \label{fig:results.laplace.corner}
\end{figure}

As we can see in Figures \ref{fig:results.laplace.regression} and
\ref{fig:results.laplace.corner}, we recover the values of the parameters that were
used to generate the dataset.

We then generated 1000 datasets from the same ground-truth parameters, and
calculated the maximum a posteriori (MAP) values of the regression coefficients
and shape parameter $\nu$. The results (illustrated in Figure
\ref{fig:results.laplace.map}) indicate that the estimate of true parameter values is
unbiased.{
\color{red} We also calculated the 95\% credible intervals for each parameter,
finding that the true parameter value was contained within the interval for
$x$\% of parameters and runs.
}

\begin{figure}
    \includegraphics[width=\columnwidth]{example-image-a}
    \caption{The distribution of maximum a posteriori (MAP) estimates for each
    of the regression parameters, as well as the shape parameter $\nu$.}
    \label{fig:results.laplace.map}
\end{figure}


\subsection{Lognormally-distributed data}
\label{sec:results.lognormal}

For the final test in this section, we used an asymmetric distribution to assess
performance of the model in this regime.

We generate $N = 25$ datapoints under the following model:
\begin{eqnarray}
    x_i &\sim& \mathcal U (-5, 5) \\
    \log_{10} y_i &\sim& \mathcal N (\mu = \log_{10} (4 + 8 x_i), \sigma = 0.2) \\
    \log_{10} \sigma_{x, i} &\sim& \mathcal N (\mu = -1, \sigma = 0.1) \\
    \log_{10} \sigma_{y, i} &\sim& \mathcal N (\mu = -1, \sigma = 0.1) \\
    \hat{x}_i &\sim& \mathcal N (\mu = x_i, \sigma = \sigma_{x, i}) \\
    \hat{y}_i &\sim& \mathcal N (\mu = y_i, \sigma = \sigma_{y, i}).
\end{eqnarray}

\begin{figure}
    \includegraphics[width=\columnwidth]{example-image-a}
    \caption{{\color{red} 100} draws from the posterior of regression lines
    (red) for the lognormally-distributed dataset (black points). The ground
    truth regression line is illustrated by the black dashed line.}
    \label{fig:results.lognormal.regression}
\end{figure}

\begin{figure}
    \includegraphics[width=\columnwidth]{example-image-a}
    \caption{The posterior for each of the regression coefficients and the shape
    parameter $\nu$ for the lognormally-distributed dataset, with ground-truth
    values indicated by the black dashed lines.}
    \label{fig:results.lognormal.corner}
\end{figure}

As we can see in Figures \ref{fig:results.lognormal.regression} and
\ref{fig:results.lognormal.corner}, we recover the values of the parameters that were
used to generate the dataset.

We then generated 1000 datasets from the same ground-truth parameters, and
calculated the maximum a posteriori (MAP) values of the regression coefficients
and shape parameter $\nu$. The results (illustrated in Figure
\ref{fig:results.lognormal.map}) indicate that the estimate of true parameter values is
unbiased.{
\color{red} We also calculated the 95\% credible intervals for each parameter,
finding that the true parameter value was contained within the interval for
$x$\% of parameters and runs.
}

\begin{figure}
    \includegraphics[width=\columnwidth]{example-image-a}
    \caption{The distribution of maximum a posteriori (MAP) estimates for each
    of the regression parameters, as well as the shape parameter $\nu$.}
    \label{fig:results.lognormal.map}
\end{figure}


\section{Demonstration on real-world data}
\label{sec:real-world}

A simple demonstration on real data (Park is a possibility; another is an
M-sigma analysis; possibly the data from the Kelly paper).  Suggest trying this
on the paper used by Kelly as can directly compare.

% % Example figure
% \begin{figure}
% 	% To include a figure from a file named example.*
% 	% Allowable file formats are eps or ps if compiling using latex
% 	% or pdf, png, jpg if compiling using pdflatex
% 	%\includegraphics[width=\columnwidth]{example}
%     Column width is \the\columnwidth.
%     Text width is \the\textwidth.
%     \caption{This is an example figure. Captions appear below each figure.
% 	Give enough detail for the reader to understand what they're looking at,
% 	but leave detailed discussion to the main body of the text.}
%     \label{fig:example_figure}
% \end{figure}

% \begin{figure*}
% 	% To include a figure from a file named example.*
% 	% Allowable file formats are eps or ps if compiling using latex
% 	% or pdf, png, jpg if compiling using pdflatex
% 	%\includegraphics[width=\columnwidth]{example}
%     \the\textwidth
%     \caption{This is an example figure. Captions appear below each figure.
% 	Give enough detail for the reader to understand what they're looking at,
% 	but leave detailed discussion to the main body of the text.}
%     \label{fig:example_widefigure}
% \end{figure*}

% % Example table
% \begin{table}
% 	\centering
% 	\caption{This is an example table. Captions appear above each table.
% 	Remember to define the quantities, symbols and units used.}
% 	\label{tab:example_table}
% 	\begin{tabular}{lccr} % four columns, alignment for each
% 		\hline
% 		A & B & C & D\\
% 		\hline
% 		1 & 2 & 3 & 4\\
% 		2 & 4 & 6 & 8\\
% 		3 & 5 & 7 & 9\\
% 		\hline
% 	\end{tabular}
% \end{table}


\section{Conclusions}

Summarise the take-home point that the method is a form of robust inference;
works on all sorts of outliers; then lead on to use in Paper 2 on the mass
ladder.

\section*{Acknowledgements}

The Acknowledgements section is not numbered. Here you can thank helpful
colleagues, acknowledge funding agencies, telescopes and facilities used etc.
Try to keep it short.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Data Availability}


The inclusion of a Data Availability Statement is a requirement for articles
published in MNRAS. Data Availability Statements provide a standardised format
for readers to understand the availability of data underlying the research
results described in the article. The statement may refer to original data
generated in the course of the study or to third-party data analysed in the
article. The statement should describe and provide means of access, where
possible, by linking to the data or providing the required accession numbers for
the relevant databases or DOIs.




%%%%%%%%%%%%%%%%%%%% REFERENCES %%%%%%%%%%%%%%%%%%

% The best way to enter references is to use BibTeX:

\bibliographystyle{mnras}
\bibliography{references}


% Alternatively you could enter them by hand, like this:
% This method is tedious and prone to error if you have lots of references
%\begin{thebibliography}{99}
%\bibitem[\protect\citeauthoryear{Author}{2012}]{Author2012}
%Author A.~N., 2013, Journal of Improbable Astronomy, 1, 1
%\bibitem[\protect\citeauthoryear{Others}{2013}]{Others2013}
%Others S., 2012, Journal of Interesting Stuff, 17, 198
%\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% APPENDICES %%%%%%%%%%%%%%%%%%%%%

\appendix

\section{Prior choice for shape parameter}

Previous work that has employed the $t$-distributions has used different
priors for the shape parameter $\nu$, which controls how heavy-tailed the
distribution is. Previous priors suggested include:
\begin{itemize}
    \item priors of the form $\nu \sim {\rm Inv-}\Gamma(\alpha, \beta)$ for some
          $\left\{\alpha, \beta\right\}$ -- e.g.
          \citet{Juarez:2010} uses $\left\{\alpha = 2, \beta = 10\right\}$;
          \citet{Ding:2014} uses $\left\{\alpha = 1, \beta = 10\right\}$
    \item A uniform prior in $\frac1\nu \sim U(0, 1)$ \citep{Gelman:2013}
    \item A uniform prior in distribution peak height relative to a normal
    distribution, such that
    \begin{equation}
        \sqrt{\frac2\nu}\frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)} \sim U(0, 1).
    \end{equation}
\end{itemize}
These priors are illustrated in Figure \ref{fig:priors.pdf}.

\begin{figure}
	\includegraphics[width=\columnwidth]{graphics/pdf_nu}
    \caption{This is an example figure. Captions appear below each figure.
	Give enough detail for the reader to understand what they're looking at,
	but leave detailed discussion to the main body of the text.}
    \label{fig:priors.pdf}
\end{figure}

A uniform proper prior in $\frac1\nu$ gives a hard cutoff in $\nu$, which
renders regions of parameter space inaccessible (in the case of the
\citet{Gelman:2013} prior, $\nu = 1$ is the cutoff, which corresponds to a
Cauchy distribution.) On the other hand, the prior used in \citet{Feeney:2018}
has disproportionate prior density at low values of $\nu$, which corresponds to
models with outliers several orders of magnitude larger than the predicted
effect size.

It can also be instructive to calculate the ``outlier fraction'' for different
values of $\nu$. We define the outlier fraction $\omega$ as
\begin{eqnarray}
    \omega(\nu) &=& \mathcal P\left(|x - \mu| > 3 \sigma \mid x \sim t_\nu (\mu, \sigma^2) \right) \\
    &=& I\left(\frac{\nu}{\nu + 3^2};\frac\nu2, \frac12\right),
\end{eqnarray}
where $I(x; \alpha, \beta)$ is the regularised beta function. For a normal
distribution, we expect about 0.27\% of points to be outliers greater than
3~$\sigma$ from the mean --- i.e. $\lim_{\nu \rightarrow \infty}\omega(\nu)
\approx 0.0027$; for a Cauchy distribution, $\omega(\nu = 1) \approx 0.20$.

The cumulative distribution functions for the priors in terms of outlier
fraction are illustrated in Figure \ref{fig:priors.outlier_frac}.
\begin{figure}
	\includegraphics[width=\columnwidth]{graphics/cdf_outlier_frac}
    \caption{This is an example figure. Captions appear below each figure.
	Give enough detail for the reader to understand what they're looking at,
	but leave detailed discussion to the main body of the text.}
    \label{fig:priors.outlier_frac}
\end{figure}

In this work, we use the prior
\begin{equation}
    \nu \sim {\rm Inv-}\Gamma(3, 10),
\end{equation}
where ${\rm Inv-}\Gamma(\alpha, \beta)$ is an inverse gamma distribution with
{\color{red} shape parameter $\alpha$ and scale parameter $\beta$}.

This prior was chosen for two reasons:
\begin{itemize}
    \item 
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Don't change these lines
\bsp	% typesetting comment
\label{lastpage}
\end{document}

% End of mnras_template.tex
